---
title: "Practical Machine Learning - Course Project"
author: "Georg Vogelhuber"
date: "16. Juni 2015"
output: 
  html_document: 
    keep_md: yes
---

# Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement  a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

# Goal
The goal of this project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. We use all of the available variables to predict with. 

More detailed information on the data for this project is available from the website http://groupware.les.inf.puc-rio.br/har.

# Preliminary steps

First we load the necessary libraries and set the random seed to ensure reproducability.

```{r, message=FALSE}
library(Amelia)   
library(dplyr)
library(ggplot2)
library(caret)
library(lubridate)
library(doParallel)
library(rattle)

set.seed(11111)
```

Next we read training and testing data sets:
```{r}
training_data <- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
testing_data <- read.csv("pml-testing.csv", na.strings = c("NA", "", "#DIV/0!"))
```

The missmap-plot from the Amelia-packages shows that there are several columns 
containing mostly NA values:
```{r}
missmap(training_data)
```

From the provided webpage and related papers, it is clear that the data is collected in rolling, overlapping time windows, and that the rows with `new_window == 'yes'` holds the aggregates of either the preceeding or following rows.  As there are only
```{r}
sum(training_data$new_window=="yes")
```
of these rows, we remove them from our training data.
```{r}
training_data %>% filter(new_window=="no") -> training_data
```

After removing these rows, there are columns made up only of NA values. We also remove
these columns:
```{r}
nr_rows <- dim(training_data)[[1]]
na_count <- apply(X = training_data, 2, function(x) {sum(is.na(x))})

training_data <- tbl_df(training_data[, !(na_count == nr_rows)])
training_data %>% select(-new_window, -X) -> training_data
```

Finally the data-values in the `cvtd_timestamp` column are parsed into the date-format.
```{r}
training_data %>% mutate(cvtd_timestamp= dmy_hm(cvtd_timestamp)) -> training_data
testing_data %>% mutate(cvtd_timestamp= dmy_hm(cvtd_timestamp)) -> testing_data
```

Now we split the training data into a training and a cross validation set:
```{r}
in_train <- createDataPartition(training_data$classe, p = 0.75, list=FALSE)
training <- training_data[in_train,]
cross_validation <- training_data[-in_train,]
```

# Train Random Forest

We use a random forest classifier to build our model, because a random forrest can handle different classes for prediction very well
and has also provides good accuracy. We use a 10-fold cross-validation to train the random forrest on the `training` data.

```{r, cache=TRUE}
fit_control <- trainControl(
  method = "cv",
  number = 10
  )

cl <- makeCluster(3)
registerDoParallel(cl)

model_fit <- train(classe~.,
                   data=training,
                   method="rf",
                   trControl=fit_control,
                   allowParallel=TRUE )
stopCluster(cl)

```

The final model:
```{r}
model_fit
```

```{r}
summary(model_fit$finalModel)
```

# Model evaluation

The following variables are the most important features for the final model:
```{r, message=FALSE}
library(randomForest)
varImpPlot(model_fit$finalModel)
```
As we can see, the predicted class depends heavily on the time the exercice was executed. So
for a meaningful model one should exclude the timestamp variables. As this was not asked in
the project assignment, we keep this model.

Now we check the accuracy on the training and cross validation set:
```{r}
train_cm <- confusionMatrix(training$classe, predict(model_fit, training))
train_cm
```

```{r}
cross_cm <- confusionMatrix(cross_validation$classe, predict(model_fit, cross_validation))
cross_cm
```

# Sample Error

The accuracy for the training set is in the 95% confidence intervall
```{r}
c(train_cm$overall[3],train_cm$overall[4])
```
So the training classification error has a 95% confidence intervall of
```{r}
ci <- c(1-train_cm$overall[4],1-train_cm$overall[3])
names(ci) <- NULL
ci
```

The accuracy for the cross validation set is in the 95% confidence intervall
```{r}
c(cross_cm$overall[3],cross_cm$overall[4])
```
So the out of sample classification error has a 95% confidence intervall of
```{r}
ci <- c(1-cross_cm$overall[4],1-cross_cm$overall[3])
names(ci) <- NULL
ci
```

The estimated value for the out of sample classification error is:
```{r}
err <- 1-cross_cm$overall[1]
names(err) <- NULL
err
```
# Prediction for test set

```{r}
predict(model_fit, testing_data)
```
